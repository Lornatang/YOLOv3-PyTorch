[net]
# Testing
# batch = 1
# subdivisions = 1
# Training
batch = 64
subdivisions = 16
width = 608
height = 608
channels = 3
momentum = 0.9
decay = 0.0005
angle = 0
saturation = 1.5
exposure = 1.5
hue = .1

learning_rate = 0.001
burn_in = 1000
max_batches = 50200
policy = steps
steps = 40000,45000
scales = .1,.1

# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 24
size = 3
stride = 2
pad = 1
activation = relu

[maxpool]
size = 3
stride = 2

# layer 0
# Bottleneck 0
# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 44
size = 1
stride = 1
pad = 0
activation = relu
[convolutional]
batch_normalize = 1
filters = 44
size = 3
stride = 2
pad = 1
groups = 44
activation = linear
[convolutional]
batch_normalize = 1
filters = 176
size = 1
stride = 1
pad = 0
activation = linear
[avgpool]
size = 3
stride = 2
[shortcut]
from = -2
activation = linear

# Bottleneck 1
# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 50
size = 1
stride = 1
pad = 0
groups = 2
activation = relu
[shuffle]
groups = 2
[convolutional]
batch_normalize = 1
filters = 50
size = 3
stride = 1
pad = 1
groups = 50
activation = linear
[convolutional]
batch_normalize = 1
filters = 200
size = 1
stride = 1
pad = 0
groups = 2
activation = linear

# Bottleneck 2
# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 50
size = 1
stride = 1
pad = 0
groups = 2
activation = relu
[shuffle]
groups = 2
[convolutional]
batch_normalize = 1
filters = 50
size = 3
stride = 1
pad = 1
groups = 50
activation = linear
[convolutional]
batch_normalize = 1
filters = 200
size = 1
stride = 1
pad = 0
groups = 2
activation = linear

# Bottleneck 3
# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 50
size = 1
stride = 1
pad = 0
groups = 2
activation = relu
[shuffle]
groups = 2
[convolutional]
batch_normalize = 1
filters = 50
size = 3
stride = 1
pad = 1
groups = 50
activation = linear
[convolutional]
batch_normalize = 1
filters = 200
size = 1
stride = 1
pad = 0
groups = 2
activation = linear

# layer 1
# Bottleneck 0
# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 50
size = 1
stride = 1
pad = 0
groups = 2
activation = relu
[shuffle]
groups = 2
[convolutional]
batch_normalize = 1
filters = 50
size = 3
stride = 2
pad = 1
groups = 50
activation = linear
[convolutional]
batch_normalize = 1
filters = 200
size = 1
stride = 1
pad = 0
groups = 2
activation = linear
[avgpool]
size = 3
stride = 2
[shortcut]
from = -2
activation = linear

# Bottleneck 1
# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 100
size = 1
stride = 1
pad = 0
groups = 2
activation = relu
[shuffle]
groups = 2
[convolutional]
batch_normalize = 1
filters = 100
size = 3
stride = 1
pad = 1
groups = 100
activation = linear
[convolutional]
batch_normalize = 1
filters = 400
size = 1
stride = 1
pad = 0
groups = 2
activation = linear

# Bottleneck 2
# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 100
size = 1
stride = 1
pad = 0
groups = 2
activation = relu
[shuffle]
groups = 2
[convolutional]
batch_normalize = 1
filters = 100
size = 3
stride = 1
pad = 1
groups = 100
activation = linear
[convolutional]
batch_normalize = 1
filters = 400
size = 1
stride = 1
pad = 0
groups = 2
activation = linear

# Bottleneck 3
# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 100
size = 1
stride = 1
pad = 0
groups = 2
activation = relu
[shuffle]
groups = 2
[convolutional]
batch_normalize = 1
filters = 100
size = 3
stride = 1
pad = 1
groups = 100
activation = linear
[convolutional]
batch_normalize = 1
filters = 400
size = 1
stride = 1
pad = 0
groups = 2
activation = linear

# Bottleneck 4
# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 100
size = 1
stride = 1
pad = 0
groups = 2
activation = relu
[shuffle]
groups = 2
[convolutional]
batch_normalize = 1
filters = 100
size = 3
stride = 1
pad = 1
groups = 100
activation = linear
[convolutional]
batch_normalize = 1
filters = 400
size = 1
stride = 1
pad = 0
groups = 2
activation = linear

# Bottleneck 5
# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 100
size = 1
stride = 1
pad = 0
groups = 2
activation = relu
[shuffle]
groups = 2
[convolutional]
batch_normalize = 1
filters = 100
size = 3
stride = 1
pad = 1
groups = 100
activation = linear
[convolutional]
batch_normalize = 1
filters = 400
size = 1
stride = 1
pad = 0
groups = 2
activation = linear

# Bottleneck 6
# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 100
size = 1
stride = 1
pad = 0
groups = 2
activation = relu
[shuffle]
groups = 2
[convolutional]
batch_normalize = 1
filters = 100
size = 3
stride = 1
pad = 1
groups = 100
activation = linear
[convolutional]
batch_normalize = 1
filters = 400
size = 1
stride = 1
pad = 0
groups = 2
activation = linear

# Bottleneck 7
# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 100
size = 1
stride = 1
pad = 0
groups = 2
activation = relu
[shuffle]
groups = 2
[convolutional]
batch_normalize = 1
filters = 100
size = 3
stride = 1
pad = 1
groups = 100
activation = linear
[convolutional]
batch_normalize = 1
filters = 400
size = 1
stride = 1
pad = 0
groups = 2
activation = linear

# layer 2
# Bottleneck 0
# ConvBNReLU
[convolutional]
batch_normalize = 1
in_features = 400
filters = 100
size = 1
stride = 1
pad = 0
groups = 2
activation = relu
[shuffle]
groups = 2
[convolutional]
batch_normalize = 1
filters = 100
size = 3
stride = 2
pad = 1
groups = 100
activation = linear
[convolutional]
batch_normalize = 1
filters = 400
size = 1
stride = 1
pad = 0
groups = 2
activation = linear
[avgpool]
size = 3
stride = 2
[shortcut]
from = -2
activation = linear

# Bottleneck 1
# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 200
size = 1
stride = 1
pad = 0
groups = 2
activation = relu
[shuffle]
groups = 2
[convolutional]
batch_normalize = 1
filters = 200
size = 3
stride = 1
pad = 1
groups = 200
activation = linear
[convolutional]
batch_normalize = 1
filters = 800
size = 1
stride = 1
pad = 0
groups = 2
activation = linear

# Bottleneck 2
# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 200
size = 1
stride = 1
pad = 0
groups = 2
activation = relu
[shuffle]
groups = 2
[convolutional]
batch_normalize = 1
filters = 200
size = 3
stride = 1
pad = 1
groups = 200
activation = linear
[convolutional]
batch_normalize = 1
filters = 800
size = 1
stride = 1
pad = 0
groups = 2
activation = linear

# Bottleneck 3
# ConvBNReLU
[convolutional]
batch_normalize = 1
filters = 200
size = 1
stride = 1
pad = 0
groups = 2
activation = relu
[shuffle]
groups = 2
[convolutional]
batch_normalize = 1
filters = 200
size = 3
stride = 1
pad = 1
groups = 200
activation = linear
[convolutional]
batch_normalize = 1
filters = 800
size = 1
stride = 1
pad = 0
groups = 2
activation = linear

######################

[convolutional]
batch_normalize = 1
filters = 512
size = 1
stride = 1
pad = 1
activation = leaky

[convolutional]
batch_normalize = 1
size = 3
stride = 1
pad = 1
filters = 1024
activation = leaky

[convolutional]
batch_normalize = 1
filters = 512
size = 1
stride = 1
pad = 1
activation = leaky

[convolutional]
batch_normalize = 1
size = 3
stride = 1
pad = 1
filters = 1024
activation = leaky

[convolutional]
batch_normalize = 1
filters = 512
size = 1
stride = 1
pad = 1
activation = leaky

[convolutional]
batch_normalize = 1
size = 3
stride = 1
pad = 1
filters = 1024
activation = leaky

[convolutional]
size = 1
stride = 1
pad = 1
filters = 255
activation = linear

# prediction large object
# 13 * 13
[yolo]
mask = 6,7,8
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes = 80
num = 9
jitter = .3
ignore_thresh = .7
truth_thresh = 1
random = 1

[route]
layers = -4

[convolutional]
batch_normalize = 1
filters = 256
size = 1
stride = 1
pad = 1
activation = leaky

[upsample]
stride = 2

[route]
layers = -1, 20

[convolutional]
batch_normalize = 1
filters = 256
size = 1
stride = 1
pad = 1
activation = leaky

[convolutional]
batch_normalize = 1
size = 3
stride = 1
pad = 1
filters = 512
activation = leaky

[convolutional]
batch_normalize = 1
filters = 256
size = 1
stride = 1
pad = 1
activation = leaky

[convolutional]
batch_normalize = 1
size = 3
stride = 1
pad = 1
filters = 512
activation = leaky

[convolutional]
batch_normalize = 1
filters = 256
size = 1
stride = 1
pad = 1
activation = leaky

[convolutional]
batch_normalize = 1
size = 3
stride = 1
pad = 1
filters = 512
activation = leaky

[convolutional]
size = 1
stride = 1
pad = 1
filters = 255
activation = linear

# prediction medium object
# 26 * 26
[yolo]
mask = 3,4,5
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes = 80
num = 9
jitter = .3
ignore_thresh = .7
truth_thresh = 1
random = 1

[route]
layers = -4

[convolutional]
batch_normalize = 1
filters = 128
size = 1
stride = 1
pad = 1
activation = leaky

[upsample]
stride = 2

[route]
layers = -1, 4

[convolutional]
batch_normalize = 1
filters = 128
size = 1
stride = 1
pad = 1
activation = leaky

[convolutional]
batch_normalize = 1
size = 3
stride = 1
pad = 1
filters = 256
activation = leaky

[convolutional]
batch_normalize = 1
filters = 128
size = 1
stride = 1
pad = 1
activation = leaky

[convolutional]
batch_normalize = 1
size = 3
stride = 1
pad = 1
filters = 256
activation = leaky

[convolutional]
batch_normalize = 1
filters = 128
size = 1
stride = 1
pad = 1
activation = leaky

[convolutional]
batch_normalize = 1
size = 3
stride = 1
pad = 1
filters = 256
activation = leaky

[convolutional]
size = 1
stride = 1
pad = 1
filters = 255
activation = linear

# prediction small object
# 52 * 52
[yolo]
mask = 0,1,2
anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
classes = 80
num = 9
jitter = .3
ignore_thresh = .7
truth_thresh = 1
random = 1